{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import shutil as sh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data\"\n",
    "path2 = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all numbers with \"NUM\" special character\n",
    "num_dec = re.compile(\"[\\d]+\\.[\\d]+\")\n",
    "num_comma = re.compile(\"[\\d]+,[\\d]+\")\n",
    "num_reg = re.compile(\"[\\d]+\")\n",
    "\n",
    "# Detect non-English characters\n",
    "non_eng = re.compile(\"[^a-zA-Z\\n ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path,filename), \"r\", encoding=\"iso-8859-15\") as file:\n",
    "        try:\n",
    "            text = file.read().lower()\n",
    "        except UnicodeDecodeError:\n",
    "            print(filename)\n",
    "\n",
    "        # Replace various forms of numbers with NUM character\n",
    "        text = num_dec.sub(\"NUM\", text)\n",
    "        text = num_comma.sub(\"NUM\", text)\n",
    "        text = num_reg.sub(\"NUM\", text)\n",
    "       \n",
    "        # Replace all non-English characters with a space\n",
    "        text = non_eng.sub(\" \", text)\n",
    "        \n",
    "        # Replace double spaces with a single space\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        \n",
    "        with open(os.path.join(path2, filename), \"w+\") as file2:\n",
    "            file2.write(text)\n",
    "            file2.close()\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 39294628\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"/home/yingjie/Desktop/Machine_Learning/Assignment4/data_clean/train.txt\").read()\n",
    "train = [train[0:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad short reviews with a PADWORD which I expect will never occur in actual text. The titles will be padded or truncated to a length of 50 words.\n",
    "\n",
    "words that are encountered during evaluation/prediction that were not in the training dataset will be replaced by <UNK>, so that is also part of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 50\n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# create vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(train)\n",
    "with gfile.Open('vocab.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
    "        f.write(\"{}\\n\".format(word))\n",
    "N_WORDS = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert reviews to a set of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Two different brand'.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear net --> [4 5 6]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "numbers = table.lookup(tf.constant('Two different brand'.lower().split()))\n",
    "with tf.Session() as sess:\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"{} --> {}\".format(train[0], numbers.eval()))\n",
    "    #print(numbers.eval())\n",
    "    list = numbers.eval().tolist()\n",
    "    print(list[len('Two different brand'.lower().split())-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXW\r\n",
      "<UNK>\r\n",
      "we\r\n",
      "brought\r\n",
      "two\r\n",
      "different\r\n",
      "brand\r\n",
      "switches\r\n",
      "to\r\n",
      "test\r\n",
      "this\r\n",
      "srwNUM\r\n",
      "from\r\n",
      "linksys\r\n",
      "and\r\n",
      "wsrNUM\r\n",
      "netgear\r\n",
      "net\r\n"
     ]
    }
   ],
   "source": [
    "!cat vocab.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles= [b'we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear net'] (1,)\n",
      "words= SparseTensorValue(indices=array([[ 0,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0,  2],\n",
      "       [ 0,  3],\n",
      "       [ 0,  4],\n",
      "       [ 0,  5],\n",
      "       [ 0,  6],\n",
      "       [ 0,  7],\n",
      "       [ 0,  8],\n",
      "       [ 0,  9],\n",
      "       [ 0, 10],\n",
      "       [ 0, 11],\n",
      "       [ 0, 12],\n",
      "       [ 0, 13],\n",
      "       [ 0, 14],\n",
      "       [ 0, 15],\n",
      "       [ 0, 16]]), values=array([b'we', b'brought', b'two', b'different', b'brand', b'switches',\n",
      "       b'to', b'test', b'this', b'srwNUM', b'from', b'linksys', b'and',\n",
      "       b'wsrNUM', b'from', b'netgear', b'net'], dtype=object), dense_shape=array([ 1, 17]))\n",
      "dense= [[b'we' b'brought' b'two' b'different' b'brand' b'switches' b'to' b'test'\n",
      "  b'this' b'srwNUM' b'from' b'linksys' b'and' b'wsrNUM' b'from'\n",
      "  b'netgear' b'net']] (?, ?)\n",
      "numbers= [[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 12 16 17]] (?, ?)\n",
      "padding= [[ 0  0]\n",
      " [ 0 50]] (2, 2)\n",
      "padded= [[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 12 16 17  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]] (?, ?)\n",
      "sliced= [[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 12 16 17  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]] (?, 50)\n",
      "[[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 12 16 17]]\n"
     ]
    }
   ],
   "source": [
    "# string operations\n",
    "reviews = tf.constant(train)\n",
    "words = tf.string_split(reviews)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "# now pad out with zeros and then slice to constant length\n",
    "padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "padded = tf.pad(numbers, padding)\n",
    "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"titles=\", reviews.eval(), reviews.shape)\n",
    "    print (\"words=\", words.eval())\n",
    "    print (\"dense=\", densewords.eval(), densewords.shape)\n",
    "    print (\"numbers=\", numbers.eval(), numbers.shape)\n",
    "    print (\"padding=\", padding.eval(), padding.shape)\n",
    "    print (\"padded=\", padded.eval(), padded.shape)\n",
    "    print (\"sliced=\", sliced.eval(), sliced.shape)\n",
    "    print(numbers.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-b75a60b53e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \"\"\"\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4742\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4743\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4744\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   4745\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4746\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
