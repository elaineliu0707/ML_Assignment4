{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read data & basic text prerpocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import shutil as sh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data/\"\n",
    "#path2 = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data_clean/\"\n",
    "\n",
    "path = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data/\"\n",
    "path2 = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocess(path,thefile):\n",
    "    # Replace all numbers with \"NUM\" special character\n",
    "    num_dec = re.compile(\"[\\d]+\\.[\\d]+\")\n",
    "    num_comma = re.compile(\"[\\d]+,[\\d]+\")\n",
    "    num_reg = re.compile(\"[\\d]+\")\n",
    "\n",
    "    # Detect non-English characters\n",
    "    non_eng = re.compile(\"[^a-zA-Z\\n ]\")\n",
    "    \n",
    "    #open file\n",
    "    file=open(os.path.join(path,thefile), \"r\", encoding=\"iso-8859-15\")\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    \n",
    "    # Replace various forms of numbers with NUM character\n",
    "    text = num_dec.sub(\"NUM\", text)\n",
    "    text = num_comma.sub(\"NUM\", text)\n",
    "    text = num_reg.sub(\"NUM\", text)\n",
    "\n",
    "    # Replace all non-English characters with a space\n",
    "    text = non_eng.sub(\" \", text)\n",
    "\n",
    "    # Replace double spaces with a single space\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    \n",
    "    # Split the file into reviews based on \\n\n",
    "    text = text.split(\"\\n\")\n",
    "        \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train=data_preprocess(path,\"train.txt\")\n",
    "valid=data_preprocess(path,\"valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create vocabulary\n",
    "\n",
    "reference source:https://stackoverflow.com/questions/40661684/tensorflow-vocabularyprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "source": [
    "import numpy as np\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 39,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "['we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear netgear switch is over NUM and this one is under NUM both of them are all gigabit ports linksys has NUM gigabit ports and netgear has NUM gigabit ports however this srwNUM has more functions and features than the netgear NUM switches for example this switch supports lags for gang ports together in different lacp groups netgear switch does not this switch allowing setup stp on top the lags netgear does not this switch loaded with current firmware netgear switch comes with a two years old firmware this switch is really good bargain for its functions features and speed it is a shining star comparing to all those overpriced competitors the only problem we have is that this switch generates some kind of apple talk probing when we have switch setup to do stp on top of lags that probing caused problem to our routers and vpn servers so we had to turn the stp off  END what needs to be said really that hasn t already been said a million times this is one of the greatest albums of all time  and arguably dylan s best i say it is but everyone has their favorites and with an artist like dylan it s hard to pin down one album as his best either way  if you dont own this you should be ashamed take a hint from jack black in high fidelity  if you dont own it dont let anyone know as for the audiophile discussion that s been popping up  this sacd hybrid sounds spectacular and if you re one of those people that really like good sound it s worth a repurchase even if it is a bit pricey however if you truly want the best sound for this album  go with the NUM gram mono mix reissue from sundazed vinyl on a good turntable nothing digital can hold a candle to it but whatever your poison this is a timeless classic that only gets better every time you hear it six words of advice  buy it buy it buy it  END these photo corners are not as metallic as they are grey and gold not much sheen  however they stay put much better than those truly metallic ones that you have to wet  END i can t believe there aren t any reviews for this yet but i must say that this is one of the top thrash albums of the NUMs and almost certainly the best of NUM only death s scream bloody gore could give this one a run for its money during that year i love everything about this disc the playing is tight the vox are cool  the lyrics actually make you think even the mix on the album is quite good the cd is impossible to find these days but i just heard some great news brazil s marquee records has signed a deal with the legendary canadian thrashers sacrifice for the re release of their first three albums NUM s torment in fire  NUM s forward to termination and NUM s soldiers of misfortune  plus a live album with a complete show from NUM all cds will be double cds special editions full of bonus tracks the live one will probably be released as a cd dvd set the distribution in the u s and japan will be handled by khaosmaster records and disk heaven respectively metal lives   END i have been trying to move away from fake sugars bad for my memory  but stevia s mild licorice taste really bothers me i m hyper sensitive to licorice since i hate it so much probably others would be fine with it  so i picked up this light agave syrup from a health food grocery store i just made my first cup of tea with it and i have to say i really like it it s a subtly different flavor than white sugar i d say your initial thought is slightly toward honey even though it doesn t have honey s strong taste it s a softer flavor than white sugar or fake sugar somehow in all a very pleasant taste and i will certainly tell the office baker about it   END well i had bought this game years ago but was missing some cd s so i decided to buy it again the game is kinda cool you are the captian of a bird of pray how cool you can cloak your ship transfer power to repairs and fire primary and seconary weapons when you take on a star ship it can take a little skill getting your targeting lined up the star ships move all over the place fast this is an older game and you may have some problems with graphics for me it is on the start up page over all it is a good game especially if you like star trek and the klingons END this new two disc dvd set is well worth adding to any NUM fan s collection if only to get the second disc which has all of the interviews and featurettes the recorded no video interview with stanley kubrick also is extremely interesting particularly if you re a film buff toward the end kubrick speaks glowingly of arthur clarke which is refreshing considering that at the time of the interview he was keeping arthur at the end of his financial tether the disc with the film is intriguing because of the sporadic commentary provided by lockwood and dullea but i consider it a grave mistake not to have included additional commentary from others involved in the production i am grateful that we have any dvd commentary at all of course but many of gary lockwood s comments aren t especially pertinent and both are left to flail and misremember information which they were not fully prepared to conjure up or reference we do feel though that by the end we ve gotten to know these two performers better and that in itself is enjoyable i haven t made any inquiries but it strikes me as reprehensible  and worthy of some sort of explanation  that the commentary track doesn t include material from arthur c clarke daniel richter moonwatcher  the special effects wizard douglas trumbull or frederick ordway who served as the technical science consultant to kubrick all of these gentlemen are still with us and no doubt would have been glad to be involved fortunately we still meet and hear them all on disc NUM but what wonderful expertise we could have been treated to while watching the film if we could hear say richter commenting on the dawn of man sequence instead of lockwood and dullea neither of whom were involved directly it also would have been a service to include comments at length from one or more noted film critics or historians who could have put the entire film  and notable sequences  into historical perspective from a cinematic viewpoint on disc NUM roger ebert does provide some thoughts but they strike me as brief and cursory still this dvd set is as good as it s going to get for awhile and  i repeat  i m glad to have it though the initial sequence shows the moon too dark to be visible at all in the theater it was quite clear  the image of the film in general looks terrific and includes colors i haven t seen in years and yet because of the glaringly missed opportunities the commentary track represents i feel as though more remains to be done dare we hope for an improved commentary track on the NUM or NUM disc criterion collection set someday    END i freely admit i was a huge fan of this show after watching the first few episodes it seemed to promise action plus just enough human feeling to make the lead character a little more than another cartoon cutout adaption i m now up to episode seven   and am almost completely over this whole series what started as an action packed vigilante against the strong men of crime idea promising some vicarious entertainment as we watch our hero take out the baddies in highly imaginative ways   progressively degenerates into lengthy soap ridden clich eacute scenes of feminine talkfests about caring  female relationships issues support and other assorted warm fuzzy aspects this is all done with such over emotional bad acting that it just overwhelms the original concept completely it is literally as if whole sections of the bold  the beautiful have been spliced into the episodes   complete with that oh so serious method of acting   where they exchange those deep  meaningfuls in near whispers with grim faces bad bad   and even worse it s exactly what the writers did to dexter with that damn plot line of the baby and all those family issues NUM of the viewers just wanted to see dexter carve up the baddies   not change damn dirty diapers and go all daddy fied then they had to resort to some unbelievably contrived writing to try and wrest back some of the initial excitement as the fans fell away i can only imagine these fluffy girly themes are being introduced to arrow as an attempt to attract more female viewers to the genre but it has totally ruined what promised to be a pretty good park your brain in neutral and enjoy fast moving concept it s now bogged down in over emotional baggage   with far too many over earnest female characters all competing for their emotional attention as they emote breathily in close up i ve stuck it out this far but the soap is now getting so thick and the action so infrequent that it s very hard to see the original concept emerging from the suds down to one star now   and fading rapidly for me  END an excellent examination of consumer culture and the way that corporate america has tried to deal with understand and co opt youth culture or did youth culture co opt advertising  frank gets to the bottom of it all in an always entertaining look at advertising from the madison avenue years through the sixties his examinations of various ad campaigns  such as volvo who insisted in their ads that their cars were ugly and at least not as filled with defects as the cars they used to make  are insightful and well researched in fact this book is a necessary primer for anyone doing research on youth culture it helped to change the way that i think about these issues and has become a text that i refer to often  END i bought this album mainly for my interest in the supporting cast i m a fan of mehldau redman grenadier and ballard  i had never heard kr before and i am thoroughly impressed not only does he have amazing chops but he also has a penchant for playing very quirky yet haunting melody lines almost like miles did on trumpet the song selection is excellent i think i only skip one or two of the songs and that s more because i can t wait to get to other ones deeper in the album some of this album is slow and poignant some is swinging some is afro sassy but it s never boring if you like good jazz buy this album i can t wait to get more of his stuff highlights for me are th cross song NUM song NUM and the final track NUM but again they re all quite nice ps ali jackson is one bad mutha as well  END ']"
      ]
     },
     "execution_count": 35,
=======
       "['we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear netgear switch is over NUM and this one is under NUM both of them are all gigabit ports linksys has NUM gigabit ports and netgear has NUM gigabit ports however this srwNUM has more functions and features than the netgear NUM switches for example this switch supports lags for gang ports together in different lacp groups netgear switch does not this switch allowing setup stp on top the lags netgear does not this switch loaded with current firmware netgear switch comes with a two years old firmware this switch is really good bargain for its functions features and speed it is a shining star comparing to all those overpriced competitors the only problem we have is that this switch generates some kind of apple talk probing when we have switch setup to do stp on top of lags that probing caused problem to our routers and vpn servers so we had to turn the stp off  END what needs to be said really that hasn t already been said a million times this is one of the greatest albums of all time  and arguably dylan s best i say it is but everyone has their favorites and with an artist like dylan it s hard to pin down one album as his best either way  if you dont own this you should be ashamed take a hint from jack black in high fidelity  if you dont own it dont let anyone know as for the audiophile discussion that s been popping up  this sacd hybrid sounds spectacular and if you re one of those people that really like good sound it s worth a repurchase even if it is a bit pricey however if you truly want the best sound for this album  go with the NUM gram mono mix reissue from sundazed vinyl on a good turntable nothing digital can hold a candle to it but whatever your poison this is a timeless classic that only gets better every time you hear it six words of advice  buy it buy it buy it  END ']"
      ]
     },
     "execution_count": 39,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "num_of_reviews=10\n",
=======
    "num_of_reviews=2\n",
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
    "small_train=''\n",
    "for i in range(num_of_reviews):\n",
    "    small_train+=train[i]+\" END \"\n",
    "small_train=[small_train]\n",
    "small_train"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 40,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "2011"
      ]
     },
     "execution_count": 36,
=======
       "370"
      ]
     },
     "execution_count": 40,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_length=len(small_train[0].split(\" \"))\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 41,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
=======
   "execution_count": 42,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[1, 2, 3, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
=======
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  93,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  93,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 42,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform the documents using the vocabulary.\n",
    "train_transformed = np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 43,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[array([  1,   2,   3, ..., 124, 342,  93])]"
      ]
     },
     "execution_count": 39,
=======
       "[array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  93,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  93])]"
      ]
     },
     "execution_count": 43,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
<<<<<<< HEAD
   "source": [
    "train_transformed=[np.trim_zeros(train_transformed[0], 'b')]\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<UNK>', 0), ('we', 1), ('brought', 2), ('two', 3), ('different', 4), ('brand', 5), ('switches', 6), ('to', 7), ('test', 8), ('this', 9), ('srwNUM', 10), ('from', 11), ('linksys', 12), ('and', 13), ('wsrNUM', 14), ('netgear', 15), ('switch', 16), ('is', 17), ('over', 18), ('NUM', 19), ('one', 20), ('under', 21), ('both', 22), ('of', 23), ('them', 24), ('are', 25), ('all', 26), ('gigabit', 27), ('ports', 28), ('has', 29), ('however', 30), ('more', 31), ('functions', 32), ('features', 33), ('than', 34), ('the', 35), ('for', 36), ('example', 37), ('supports', 38), ('lags', 39), ('gang', 40), ('together', 41), ('in', 42), ('lacp', 43), ('groups', 44), ('does', 45), ('not', 46), ('allowing', 47), ('setup', 48), ('stp', 49), ('on', 50), ('top', 51), ('loaded', 52), ('with', 53), ('current', 54), ('firmware', 55), ('comes', 56), ('a', 57), ('years', 58), ('old', 59), ('really', 60), ('good', 61), ('bargain', 62), ('its', 63), ('speed', 64), ('it', 65), ('shining', 66), ('star', 67), ('comparing', 68), ('those', 69), ('overpriced', 70), ('competitors', 71), ('only', 72), ('problem', 73), ('have', 74), ('that', 75), ('generates', 76), ('some', 77), ('kind', 78), ('apple', 79), ('talk', 80), ('probing', 81), ('when', 82), ('do', 83), ('caused', 84), ('our', 85), ('routers', 86), ('vpn', 87), ('servers', 88), ('so', 89), ('had', 90), ('turn', 91), ('off', 92), ('END', -1), ('what', 94), ('needs', 95), ('be', 96), ('said', 97), ('hasn', 98), ('t', 99), ('already', 100), ('been', 101), ('million', 102), ('times', 103), ('greatest', 104), ('albums', 105), ('time', 106), ('arguably', 107), ('dylan', 108), ('s', 109), ('best', 110), ('i', 111), ('say', 112), ('but', 113), ('everyone', 114), ('their', 115), ('favorites', 116), ('an', 117), ('artist', 118), ('like', 119), ('hard', 120), ('pin', 121), ('down', 122), ('album', 123), ('as', 124), ('his', 125), ('either', 126), ('way', 127), ('if', 128), ('you', 129), ('dont', 130), ('own', 131), ('should', 132), ('ashamed', 133), ('take', 134), ('hint', 135), ('jack', 136), ('black', 137), ('high', 138), ('fidelity', 139), ('let', 140), ('anyone', 141), ('know', 142), ('audiophile', 143), ('discussion', 144), ('popping', 145), ('up', 146), ('sacd', 147), ('hybrid', 148), ('sounds', 149), ('spectacular', 150), ('re', 151), ('people', 152), ('sound', 153), ('worth', 154), ('repurchase', 155), ('even', 156), ('bit', 157), ('pricey', 158), ('truly', 159), ('want', 160), ('go', 161), ('gram', 162), ('mono', 163), ('mix', 164), ('reissue', 165), ('sundazed', 166), ('vinyl', 167), ('turntable', 168), ('nothing', 169), ('digital', 170), ('can', 171), ('hold', 172), ('candle', 173), ('whatever', 174), ('your', 175), ('poison', 176), ('timeless', 177), ('classic', 178), ('gets', 179), ('better', 180), ('every', 181), ('hear', 182), ('six', 183), ('words', 184), ('advice', 185), ('buy', 186), ('these', 187), ('photo', 188), ('corners', 189), ('metallic', 190), ('they', 191), ('grey', 192), ('gold', 193), ('much', 194), ('sheen', 195), ('stay', 196), ('put', 197), ('ones', 198), ('wet', 199), ('believe', 200), ('there', 201), ('aren', 202), ('any', 203), ('reviews', 204), ('yet', 205), ('must', 206), ('thrash', 207), ('NU', 208), ('Ms', 209), ('almost', 210), ('certainly', 211), ('death', 212), ('scream', 213), ('bloody', 214), ('gore', 215), ('could', 216), ('give', 217), ('run', 218), ('money', 219), ('during', 220), ('year', 221), ('love', 222), ('everything', 223), ('about', 224), ('disc', 225), ('playing', 226), ('tight', 227), ('vox', 228), ('cool', 229), ('lyrics', 230), ('actually', 231), ('make', 232), ('think', 233), ('quite', 234), ('cd', 235), ('impossible', 236), ('find', 237), ('days', 238), ('just', 239), ('heard', 240), ('great', 241), ('news', 242), ('brazil', 243), ('marquee', 244), ('records', 245), ('signed', 246), ('deal', 247), ('legendary', 248), ('canadian', 249), ('thrashers', 250), ('sacrifice', 251), ('release', 252), ('first', 253), ('three', 254), ('torment', 255), ('fire', 256), ('forward', 257), ('termination', 258), ('soldiers', 259), ('misfortune', 260), ('plus', 261), ('live', 262), ('complete', 263), ('show', 264), ('cds', 265), ('will', 266), ('double', 267), ('special', 268), ('editions', 269), ('full', 270), ('bonus', 271), ('tracks', 272), ('probably', 273), ('released', 274), ('dvd', 275), ('set', 276), ('distribution', 277), ('u', 278), ('japan', 279), ('handled', 280), ('by', 281), ('khaosmaster', 282), ('disk', 283), ('heaven', 284), ('respectively', 285), ('metal', 286), ('lives', 287), ('trying', 288), ('move', 289), ('away', 290), ('fake', 291), ('sugars', 292), ('bad', 293), ('my', 294), ('memory', 295), ('stevia', 296), ('mild', 297), ('licorice', 298), ('taste', 299), ('bothers', 300), ('me', 301), ('m', 302), ('hyper', 303), ('sensitive', 304), ('since', 305), ('hate', 306), ('others', 307), ('would', 308), ('fine', 309), ('picked', 310), ('light', 311), ('agave', 312), ('syrup', 313), ('health', 314), ('food', 315), ('grocery', 316), ('store', 317), ('made', 318), ('cup', 319), ('tea', 320), ('subtly', 321), ('flavor', 322), ('white', 323), ('sugar', 324), ('d', 325), ('initial', 326), ('thought', 327), ('slightly', 328), ('toward', 329), ('honey', 330), ('though', 331), ('doesn', 332), ('strong', 333), ('softer', 334), ('or', 335), ('somehow', 336), ('very', 337), ('pleasant', 338), ('tell', 339), ('office', 340), ('baker', 341), ('well', 342), ('bought', 343), ('game', 344), ('ago', 345), ('was', 346), ('missing', 347), ('decided', 348), ('again', 349), ('kinda', 350), ('captian', 351), ('bird', 352), ('pray', 353), ('how', 354), ('cloak', 355), ('ship', 356), ('transfer', 357), ('power', 358), ('repairs', 359), ('primary', 360), ('seconary', 361), ('weapons', 362), ('little', 363), ('skill', 364), ('getting', 365), ('targeting', 366), ('lined', 367), ('ships', 368), ('place', 369), ('fast', 370), ('older', 371), ('may', 372), ('problems', 373), ('graphics', 374), ('start', 375), ('page', 376), ('especially', 377), ('trek', 378), ('klingons', 379), ('new', 380), ('adding', 381), ('fan', 382), ('collection', 383), ('get', 384), ('second', 385), ('which', 386), ('interviews', 387), ('featurettes', 388), ('recorded', 389), ('no', 390), ('video', 391), ('interview', 392), ('stanley', 393), ('kubrick', 394), ('also', 395), ('extremely', 396), ('interesting', 397), ('particularly', 398), ('film', 399), ('buff', 400), ('end', 401), ('speaks', 402), ('glowingly', 403), ('arthur', 404), ('clarke', 405), ('refreshing', 406), ('considering', 407), ('at', 408), ('he', 409), ('keeping', 410), ('financial', 411), ('tether', 412), ('intriguing', 413), ('because', 414), ('sporadic', 415), ('commentary', 416), ('provided', 417), ('lockwood', 418), ('dullea', 419), ('consider', 420), ('grave', 421), ('mistake', 422), ('included', 423), ('additional', 424), ('involved', 425), ('production', 426), ('am', 427), ('grateful', 428), ('course', 429), ('many', 430), ('gary', 431), ('comments', 432), ('pertinent', 433), ('left', 434), ('flail', 435), ('misremember', 436), ('information', 437), ('were', 438), ('fully', 439), ('prepared', 440), ('conjure', 441), ('reference', 442), ('feel', 443), ('ve', 444), ('gotten', 445), ('performers', 446), ('itself', 447), ('enjoyable', 448), ('haven', 449), ('inquiries', 450), ('strikes', 451), ('reprehensible', 452), ('worthy', 453), ('sort', 454), ('explanation', 455), ('track', 456), ('include', 457), ('material', 458), ('c', 459), ('daniel', 460), ('richter', 461), ('moonwatcher', 462), ('effects', 463), ('wizard', 464), ('douglas', 465), ('trumbull', 466), ('frederick', 467), ('ordway', 468), ('who', 469), ('served', 470), ('technical', 471), ('science', 472), ('consultant', 473), ('gentlemen', 474), ('still', 475), ('us', 476), ('doubt', 477), ('glad', 478), ('fortunately', 479), ('meet', 480), ('wonderful', 481), ('expertise', 482), ('treated', 483), ('while', 484), ('watching', 485), ('commenting', 486), ('dawn', 487), ('man', 488), ('sequence', 489), ('instead', 490), ('neither', 491), ('whom', 492), ('directly', 493), ('service', 494), ('length', 495), ('noted', 496), ('critics', 497), ('historians', 498), ('entire', 499), ('notable', 500), ('sequences', 501), ('into', 502), ('historical', 503), ('perspective', 504), ('cinematic', 505), ('viewpoint', 506), ('roger', 507), ('ebert', 508), ('provide', 509), ('thoughts', 510), ('strike', 511), ('brief', 512), ('cursory', 513), ('going', 514), ('awhile', 515), ('repeat', 516), ('shows', 517), ('moon', 518), ('too', 519), ('dark', 520), ('visible', 521), ('theater', 522), ('clear', 523), ('image', 524), ('general', 525), ('looks', 526), ('terrific', 527), ('includes', 528), ('colors', 529), ('seen', 530), ('glaringly', 531), ('missed', 532), ('opportunities', 533), ('represents', 534), ('remains', 535), ('done', 536), ('dare', 537), ('hope', 538), ('improved', 539), ('criterion', 540), ('someday', 541), ('freely', 542), ('admit', 543), ('huge', 544), ('after', 545), ('few', 546), ('episodes', 547), ('seemed', 548), ('promise', 549), ('action', 550), ('enough', 551), ('human', 552), ('feeling', 553), ('lead', 554), ('character', 555), ('another', 556), ('cartoon', 557), ('cutout', 558), ('adaption', 559), ('now', 560), ('episode', 561), ('seven', 562), ('completely', 563), ('whole', 564), ('series', 565), ('started', 566), ('packed', 567), ('vigilante', 568), ('against', 569), ('men', 570), ('crime', 571), ('idea', 572), ('promising', 573), ('vicarious', 574), ('entertainment', 575), ('watch', 576), ('hero', 577), ('out', 578), ('baddies', 579), ('highly', 580), ('imaginative', 581), ('ways', 582), ('progressively', 583), ('degenerates', 584), ('lengthy', 585), ('soap', 586), ('ridden', 587), ('clich', 588), ('eacute', 589), ('scenes', 590), ('feminine', 591), ('talkfests', 592), ('caring', 593), ('female', 594), ('relationships', 595), ('issues', 596), ('support', 597), ('other', 598), ('assorted', 599), ('warm', 600), ('fuzzy', 601), ('aspects', 602), ('such', 603), ('emotional', 604), ('acting', 605), ('overwhelms', 606), ('original', 607), ('concept', 608), ('literally', 609), ('sections', 610), ('bold', 611), ('beautiful', 612), ('spliced', 613), ('oh', 614), ('serious', 615), ('method', 616), ('where', 617), ('exchange', 618), ('deep', 619), ('meaningfuls', 620), ('near', 621), ('whispers', 622), ('grim', 623), ('faces', 624), ('worse', 625), ('exactly', 626), ('writers', 627), ('did', 628), ('dexter', 629), ('damn', 630), ('plot', 631), ('line', 632), ('baby', 633), ('family', 634), ('viewers', 635), ('wanted', 636), ('see', 637), ('carve', 638), ('change', 639), ('dirty', 640), ('diapers', 641), ('daddy', 642), ('fied', 643), ('then', 644), ('resort', 645), ('unbelievably', 646), ('contrived', 647), ('writing', 648), ('try', 649), ('wrest', 650), ('back', 651), ('excitement', 652), ('fans', 653), ('fell', 654), ('imagine', 655), ('fluffy', 656), ('girly', 657), ('themes', 658), ('being', 659), ('introduced', 660), ('arrow', 661), ('attempt', 662), ('attract', 663), ('genre', 664), ('totally', 665), ('ruined', 666), ('promised', 667), ('pretty', 668), ('park', 669), ('brain', 670), ('neutral', 671), ('enjoy', 672), ('moving', 673), ('bogged', 674), ('baggage', 675), ('far', 676), ('earnest', 677), ('characters', 678), ('competing', 679), ('attention', 680), ('emote', 681), ('breathily', 682), ('close', 683), ('stuck', 684), ('thick', 685), ('infrequent', 686), ('emerging', 687), ('suds', 688), ('fading', 689), ('rapidly', 690), ('excellent', 691), ('examination', 692), ('consumer', 693), ('culture', 694), ('corporate', 695), ('america', 696), ('tried', 697), ('understand', 698), ('co', 699), ('opt', 700), ('youth', 701), ('advertising', 702), ('frank', 703), ('bottom', 704), ('always', 705), ('entertaining', 706), ('look', 707), ('madison', 708), ('avenue', 709), ('through', 710), ('sixties', 711), ('examinations', 712), ('various', 713), ('ad', 714), ('campaigns', 715), ('volvo', 716), ('insisted', 717), ('ads', 718), ('cars', 719), ('ugly', 720), ('least', 721), ('filled', 722), ('defects', 723), ('used', 724), ('insightful', 725), ('researched', 726), ('fact', 727), ('book', 728), ('necessary', 729), ('primer', 730), ('doing', 731), ('research', 732), ('helped', 733), ('become', 734), ('text', 735), ('refer', 736), ('often', 737), ('mainly', 738), ('interest', 739), ('supporting', 740), ('cast', 741), ('mehldau', 742), ('redman', 743), ('grenadier', 744), ('ballard', 745), ('never', 746), ('kr', 747), ('before', 748), ('thoroughly', 749), ('impressed', 750), ('amazing', 751), ('chops', 752), ('penchant', 753), ('quirky', 754), ('haunting', 755), ('melody', 756), ('lines', 757), ('miles', 758), ('trumpet', 759), ('song', 760), ('selection', 761), ('skip', 762), ('songs', 763), ('wait', 764), ('deeper', 765), ('slow', 766), ('poignant', 767), ('swinging', 768), ('afro', 769), ('sassy', 770), ('boring', 771), ('jazz', 772), ('stuff', 773), ('highlights', 774), ('th', 775), ('cross', 776), ('final', 777), ('nice', 778), ('ps', 779), ('ali', 780), ('jackson', 781), ('mutha', 782)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict['END'] = -1\n",
    "vocab_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "source": [
    "train_transformed=[np.trim_zeros(train_transformed[0], 'b')]\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
=======
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<UNK>', 0), ('we', 1), ('brought', 2), ('two', 3), ('different', 4), ('brand', 5), ('switches', 6), ('to', 7), ('test', 8), ('this', 9), ('srwNUM', 10), ('from', 11), ('linksys', 12), ('and', 13), ('wsrNUM', 14), ('netgear', 15), ('switch', 16), ('is', 17), ('over', 18), ('NUM', 19), ('one', 20), ('under', 21), ('both', 22), ('of', 23), ('them', 24), ('are', 25), ('all', 26), ('gigabit', 27), ('ports', 28), ('has', 29), ('however', 30), ('more', 31), ('functions', 32), ('features', 33), ('than', 34), ('the', 35), ('for', 36), ('example', 37), ('supports', 38), ('lags', 39), ('gang', 40), ('together', 41), ('in', 42), ('lacp', 43), ('groups', 44), ('does', 45), ('not', 46), ('allowing', 47), ('setup', 48), ('stp', 49), ('on', 50), ('top', 51), ('loaded', 52), ('with', 53), ('current', 54), ('firmware', 55), ('comes', 56), ('a', 57), ('years', 58), ('old', 59), ('really', 60), ('good', 61), ('bargain', 62), ('its', 63), ('speed', 64), ('it', 65), ('shining', 66), ('star', 67), ('comparing', 68), ('those', 69), ('overpriced', 70), ('competitors', 71), ('only', 72), ('problem', 73), ('have', 74), ('that', 75), ('generates', 76), ('some', 77), ('kind', 78), ('apple', 79), ('talk', 80), ('probing', 81), ('when', 82), ('do', 83), ('caused', 84), ('our', 85), ('routers', 86), ('vpn', 87), ('servers', 88), ('so', 89), ('had', 90), ('turn', 91), ('off', 92), ('END', -1), ('what', 94), ('needs', 95), ('be', 96), ('said', 97), ('hasn', 98), ('t', 99), ('already', 100), ('been', 101), ('million', 102), ('times', 103), ('greatest', 104), ('albums', 105), ('time', 106), ('arguably', 107), ('dylan', 108), ('s', 109), ('best', 110), ('i', 111), ('say', 112), ('but', 113), ('everyone', 114), ('their', 115), ('favorites', 116), ('an', 117), ('artist', 118), ('like', 119), ('hard', 120), ('pin', 121), ('down', 122), ('album', 123), ('as', 124), ('his', 125), ('either', 126), ('way', 127), ('if', 128), ('you', 129), ('dont', 130), ('own', 131), ('should', 132), ('ashamed', 133), ('take', 134), ('hint', 135), ('jack', 136), ('black', 137), ('high', 138), ('fidelity', 139), ('let', 140), ('anyone', 141), ('know', 142), ('audiophile', 143), ('discussion', 144), ('popping', 145), ('up', 146), ('sacd', 147), ('hybrid', 148), ('sounds', 149), ('spectacular', 150), ('re', 151), ('people', 152), ('sound', 153), ('worth', 154), ('repurchase', 155), ('even', 156), ('bit', 157), ('pricey', 158), ('truly', 159), ('want', 160), ('go', 161), ('gram', 162), ('mono', 163), ('mix', 164), ('reissue', 165), ('sundazed', 166), ('vinyl', 167), ('turntable', 168), ('nothing', 169), ('digital', 170), ('can', 171), ('hold', 172), ('candle', 173), ('whatever', 174), ('your', 175), ('poison', 176), ('timeless', 177), ('classic', 178), ('gets', 179), ('better', 180), ('every', 181), ('hear', 182), ('six', 183), ('words', 184), ('advice', 185), ('buy', 186), ('these', 187), ('photo', 188), ('corners', 189), ('metallic', 190), ('they', 191), ('grey', 192), ('gold', 193), ('much', 194), ('sheen', 195), ('stay', 196), ('put', 197), ('ones', 198), ('wet', 199), ('believe', 200), ('there', 201), ('aren', 202), ('any', 203), ('reviews', 204), ('yet', 205), ('must', 206), ('thrash', 207), ('NU', 208), ('Ms', 209), ('almost', 210), ('certainly', 211), ('death', 212), ('scream', 213), ('bloody', 214), ('gore', 215), ('could', 216), ('give', 217), ('run', 218), ('money', 219), ('during', 220), ('year', 221), ('love', 222), ('everything', 223), ('about', 224), ('disc', 225), ('playing', 226), ('tight', 227), ('vox', 228), ('cool', 229), ('lyrics', 230), ('actually', 231), ('make', 232), ('think', 233), ('quite', 234), ('cd', 235), ('impossible', 236), ('find', 237), ('days', 238), ('just', 239), ('heard', 240), ('great', 241), ('news', 242), ('brazil', 243), ('marquee', 244), ('records', 245), ('signed', 246), ('deal', 247), ('legendary', 248), ('canadian', 249), ('thrashers', 250), ('sacrifice', 251), ('release', 252), ('first', 253), ('three', 254), ('torment', 255), ('fire', 256), ('forward', 257), ('termination', 258), ('soldiers', 259), ('misfortune', 260), ('plus', 261), ('live', 262), ('complete', 263), ('show', 264), ('cds', 265), ('will', 266), ('double', 267), ('special', 268), ('editions', 269), ('full', 270), ('bonus', 271), ('tracks', 272), ('probably', 273), ('released', 274), ('dvd', 275), ('set', 276), ('distribution', 277), ('u', 278), ('japan', 279), ('handled', 280), ('by', 281), ('khaosmaster', 282), ('disk', 283), ('heaven', 284), ('respectively', 285), ('metal', 286), ('lives', 287), ('trying', 288), ('move', 289), ('away', 290), ('fake', 291), ('sugars', 292), ('bad', 293), ('my', 294), ('memory', 295), ('stevia', 296), ('mild', 297), ('licorice', 298), ('taste', 299), ('bothers', 300), ('me', 301), ('m', 302), ('hyper', 303), ('sensitive', 304), ('since', 305), ('hate', 306), ('others', 307), ('would', 308), ('fine', 309), ('picked', 310), ('light', 311), ('agave', 312), ('syrup', 313), ('health', 314), ('food', 315), ('grocery', 316), ('store', 317), ('made', 318), ('cup', 319), ('tea', 320), ('subtly', 321), ('flavor', 322), ('white', 323), ('sugar', 324), ('d', 325), ('initial', 326), ('thought', 327), ('slightly', 328), ('toward', 329), ('honey', 330), ('though', 331), ('doesn', 332), ('strong', 333), ('softer', 334), ('or', 335), ('somehow', 336), ('very', 337), ('pleasant', 338), ('tell', 339), ('office', 340), ('baker', 341), ('well', 342), ('bought', 343), ('game', 344), ('ago', 345), ('was', 346), ('missing', 347), ('decided', 348), ('again', 349), ('kinda', 350), ('captian', 351), ('bird', 352), ('pray', 353), ('how', 354), ('cloak', 355), ('ship', 356), ('transfer', 357), ('power', 358), ('repairs', 359), ('primary', 360), ('seconary', 361), ('weapons', 362), ('little', 363), ('skill', 364), ('getting', 365), ('targeting', 366), ('lined', 367), ('ships', 368), ('place', 369), ('fast', 370), ('older', 371), ('may', 372), ('problems', 373), ('graphics', 374), ('start', 375), ('page', 376), ('especially', 377), ('trek', 378), ('klingons', 379), ('new', 380), ('adding', 381), ('fan', 382), ('collection', 383), ('get', 384), ('second', 385), ('which', 386), ('interviews', 387), ('featurettes', 388), ('recorded', 389), ('no', 390), ('video', 391), ('interview', 392), ('stanley', 393), ('kubrick', 394), ('also', 395), ('extremely', 396), ('interesting', 397), ('particularly', 398), ('film', 399), ('buff', 400), ('end', 401), ('speaks', 402), ('glowingly', 403), ('arthur', 404), ('clarke', 405), ('refreshing', 406), ('considering', 407), ('at', 408), ('he', 409), ('keeping', 410), ('financial', 411), ('tether', 412), ('intriguing', 413), ('because', 414), ('sporadic', 415), ('commentary', 416), ('provided', 417), ('lockwood', 418), ('dullea', 419), ('consider', 420), ('grave', 421), ('mistake', 422), ('included', 423), ('additional', 424), ('involved', 425), ('production', 426), ('am', 427), ('grateful', 428), ('course', 429), ('many', 430), ('gary', 431), ('comments', 432), ('pertinent', 433), ('left', 434), ('flail', 435), ('misremember', 436), ('information', 437), ('were', 438), ('fully', 439), ('prepared', 440), ('conjure', 441), ('reference', 442), ('feel', 443), ('ve', 444), ('gotten', 445), ('performers', 446), ('itself', 447), ('enjoyable', 448), ('haven', 449), ('inquiries', 450), ('strikes', 451), ('reprehensible', 452), ('worthy', 453), ('sort', 454), ('explanation', 455), ('track', 456), ('include', 457), ('material', 458), ('c', 459), ('daniel', 460), ('richter', 461), ('moonwatcher', 462), ('effects', 463), ('wizard', 464), ('douglas', 465), ('trumbull', 466), ('frederick', 467), ('ordway', 468), ('who', 469), ('served', 470), ('technical', 471), ('science', 472), ('consultant', 473), ('gentlemen', 474), ('still', 475), ('us', 476), ('doubt', 477), ('glad', 478), ('fortunately', 479), ('meet', 480), ('wonderful', 481), ('expertise', 482), ('treated', 483), ('while', 484), ('watching', 485), ('commenting', 486), ('dawn', 487), ('man', 488), ('sequence', 489), ('instead', 490), ('neither', 491), ('whom', 492), ('directly', 493), ('service', 494), ('length', 495), ('noted', 496), ('critics', 497), ('historians', 498), ('entire', 499), ('notable', 500), ('sequences', 501), ('into', 502), ('historical', 503), ('perspective', 504), ('cinematic', 505), ('viewpoint', 506), ('roger', 507), ('ebert', 508), ('provide', 509), ('thoughts', 510), ('strike', 511), ('brief', 512), ('cursory', 513), ('going', 514), ('awhile', 515), ('repeat', 516), ('shows', 517), ('moon', 518), ('too', 519), ('dark', 520), ('visible', 521), ('theater', 522), ('clear', 523), ('image', 524), ('general', 525), ('looks', 526), ('terrific', 527), ('includes', 528), ('colors', 529), ('seen', 530), ('glaringly', 531), ('missed', 532), ('opportunities', 533), ('represents', 534), ('remains', 535), ('done', 536), ('dare', 537), ('hope', 538), ('improved', 539), ('criterion', 540), ('someday', 541), ('freely', 542), ('admit', 543), ('huge', 544), ('after', 545), ('few', 546), ('episodes', 547), ('seemed', 548), ('promise', 549), ('action', 550), ('enough', 551), ('human', 552), ('feeling', 553), ('lead', 554), ('character', 555), ('another', 556), ('cartoon', 557), ('cutout', 558), ('adaption', 559), ('now', 560), ('episode', 561), ('seven', 562), ('completely', 563), ('whole', 564), ('series', 565), ('started', 566), ('packed', 567), ('vigilante', 568), ('against', 569), ('men', 570), ('crime', 571), ('idea', 572), ('promising', 573), ('vicarious', 574), ('entertainment', 575), ('watch', 576), ('hero', 577), ('out', 578), ('baddies', 579), ('highly', 580), ('imaginative', 581), ('ways', 582), ('progressively', 583), ('degenerates', 584), ('lengthy', 585), ('soap', 586), ('ridden', 587), ('clich', 588), ('eacute', 589), ('scenes', 590), ('feminine', 591), ('talkfests', 592), ('caring', 593), ('female', 594), ('relationships', 595), ('issues', 596), ('support', 597), ('other', 598), ('assorted', 599), ('warm', 600), ('fuzzy', 601), ('aspects', 602), ('such', 603), ('emotional', 604), ('acting', 605), ('overwhelms', 606), ('original', 607), ('concept', 608), ('literally', 609), ('sections', 610), ('bold', 611), ('beautiful', 612), ('spliced', 613), ('oh', 614), ('serious', 615), ('method', 616), ('where', 617), ('exchange', 618), ('deep', 619), ('meaningfuls', 620), ('near', 621), ('whispers', 622), ('grim', 623), ('faces', 624), ('worse', 625), ('exactly', 626), ('writers', 627), ('did', 628), ('dexter', 629), ('damn', 630), ('plot', 631), ('line', 632), ('baby', 633), ('family', 634), ('viewers', 635), ('wanted', 636), ('see', 637), ('carve', 638), ('change', 639), ('dirty', 640), ('diapers', 641), ('daddy', 642), ('fied', 643), ('then', 644), ('resort', 645), ('unbelievably', 646), ('contrived', 647), ('writing', 648), ('try', 649), ('wrest', 650), ('back', 651), ('excitement', 652), ('fans', 653), ('fell', 654), ('imagine', 655), ('fluffy', 656), ('girly', 657), ('themes', 658), ('being', 659), ('introduced', 660), ('arrow', 661), ('attempt', 662), ('attract', 663), ('genre', 664), ('totally', 665), ('ruined', 666), ('promised', 667), ('pretty', 668), ('park', 669), ('brain', 670), ('neutral', 671), ('enjoy', 672), ('moving', 673), ('bogged', 674), ('baggage', 675), ('far', 676), ('earnest', 677), ('characters', 678), ('competing', 679), ('attention', 680), ('emote', 681), ('breathily', 682), ('close', 683), ('stuck', 684), ('thick', 685), ('infrequent', 686), ('emerging', 687), ('suds', 688), ('fading', 689), ('rapidly', 690), ('excellent', 691), ('examination', 692), ('consumer', 693), ('culture', 694), ('corporate', 695), ('america', 696), ('tried', 697), ('understand', 698), ('co', 699), ('opt', 700), ('youth', 701), ('advertising', 702), ('frank', 703), ('bottom', 704), ('always', 705), ('entertaining', 706), ('look', 707), ('madison', 708), ('avenue', 709), ('through', 710), ('sixties', 711), ('examinations', 712), ('various', 713), ('ad', 714), ('campaigns', 715), ('volvo', 716), ('insisted', 717), ('ads', 718), ('cars', 719), ('ugly', 720), ('least', 721), ('filled', 722), ('defects', 723), ('used', 724), ('insightful', 725), ('researched', 726), ('fact', 727), ('book', 728), ('necessary', 729), ('primer', 730), ('doing', 731), ('research', 732), ('helped', 733), ('become', 734), ('text', 735), ('refer', 736), ('often', 737), ('mainly', 738), ('interest', 739), ('supporting', 740), ('cast', 741), ('mehldau', 742), ('redman', 743), ('grenadier', 744), ('ballard', 745), ('never', 746), ('kr', 747), ('before', 748), ('thoroughly', 749), ('impressed', 750), ('amazing', 751), ('chops', 752), ('penchant', 753), ('quirky', 754), ('haunting', 755), ('melody', 756), ('lines', 757), ('miles', 758), ('trumpet', 759), ('song', 760), ('selection', 761), ('skip', 762), ('songs', 763), ('wait', 764), ('deeper', 765), ('slow', 766), ('poignant', 767), ('swinging', 768), ('afro', 769), ('sassy', 770), ('boring', 771), ('jazz', 772), ('stuff', 773), ('highlights', 774), ('th', 775), ('cross', 776), ('final', 777), ('nice', 778), ('ps', 779), ('ali', 780), ('jackson', 781), ('mutha', 782)])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 43,
=======
     "execution_count": 44,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict['END'] = -1\n",
    "vocab_dict.items()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
   "metadata": {},
=======
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[array([  1,   2,   3, ..., 124, 342,  -1])]"
      ]
     },
     "execution_count": 44,
=======
       "[array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  -1,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  -1])]"
      ]
     },
     "execution_count": 46,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 is in \n",
    "train_transformed2=np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "train_transformed2=[np.trim_zeros(train_transformed2[0], 'b')]\n",
    "train_transformed2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "metadata": {},
=======
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782]\n"
     ]
    }
   ],
   "source": [
    "## Sort the vocabulary dictionary on the basis of values(id).\n",
    "## Both statements perform same task.\n",
    "#sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "## word with id i goes at index i of the list.\n",
    "vocabulary = list(list(zip(*sorted_vocab))[1])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
=======
   "execution_count": 56,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1], [2]]\n",
      "[[2], [3]]\n",
      "[[3], [4]]\n",
      "[[4], [5]]\n",
      "[[5], [6]]\n",
      "Y:\n",
      " [2 3]\n",
      "[3 4]\n",
      "[4 5]\n",
      "[5 6]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "seq_size = 2\n",
    "trainX, trainY = [], []\n",
    "max_document_length=len(train_transformed2[0])\n",
    "for i in range(max_document_length): \n",
    "    #[0,1,2,3,4,5,-1,1,2,3,4,5]\n",
    "    if -1 not in train_transformed2[0][i:i+seq_size+1]:\n",
    "        trainX.append(np.expand_dims(train_transformed2[0][i:i+seq_size], axis=1).tolist())\n",
    "        trainY.append(train_transformed2[0][i+1:i+seq_size+1])\n",
    "\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:5]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:5]]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": 57,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_vocab = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b Create vocabulary through a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textlst_2_vocab(textlst,num_of_reviews=None):\n",
    "    if num_of_reviews is None:\n",
    "        num_of_reviews=len(textlst)\n",
    "        \n",
    "    small_train=''\n",
    "    for i in range(num_of_reviews):\n",
    "        small_train+=textlst[i]+\" END \"\n",
    "    small_train=[small_train]\n",
    "    \n",
    "    max_document_length=len(small_train[0].split(\" \"))\n",
    "    \n",
    "    ## Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    \n",
    "    ## Transform the documents using the vocabulary.\n",
    "    train_transformed = np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "    \n",
    "    ## Extract word:id mapping from the object.\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    vocab_dict['END'] = -1\n",
    "    \n",
    "    train_transformed2=np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "    train_transformed2=[np.trim_zeros(train_transformed2[0], 'b')]\n",
    "    \n",
    "    #sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "    sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "    ## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "    ## word with id i goes at index i of the list.\n",
    "    vocabulary = list(list(zip(*sorted_vocab))[1])\n",
    "    \n",
    "    seq_size = 2\n",
    "    trainX, trainY = [], []\n",
    "    max_document_length=len(train_transformed2[0])\n",
    "    for i in range(max_document_length): \n",
    "        #[0,1,2,3,4,5,-1,1,2,3,4,5]\n",
    "        if -1 not in train_transformed2[0][i:i+seq_size+1]:\n",
    "            trainX.append(np.expand_dims(train_transformed2[0][i:i+seq_size], axis=1).tolist())\n",
    "            trainY.append(train_transformed2[0][i+1:i+seq_size+1])\n",
    "    \n",
    "    size_vocab = len(vocabulary)\n",
    "    \n",
    "    return trainX,trainY,size_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "X:\n",
      " [[1], [2]]\n",
      "[[2], [3]]\n",
      "[[3], [4]]\n",
      "[[4], [5]]\n",
      "[[5], [6]]\n",
      "Y:\n",
      " [2 3]\n",
      "[3 4]\n",
      "[4 5]\n",
      "[5 6]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,size_vocab=textlst_2_vocab(train[0:3])\n",
    "print(size_vocab)\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:5]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF setup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 58,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": 59,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (?, 2, 1)\n",
      "ground truth shape: (?, 2)\n",
      "output from tf,nn.dynamic_rnn shape (?, 2, 100)\n",
      "reshaped output from RNN shape: (?, 2, 100)\n",
      "weights shape: (100, 783)\n",
      "bias shape (783,)\n",
      "fully connected and reshaped outputs: Tensor(\"Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "final logits shape: (?, 2, 783)\n"
     ]
    }
   ],
   "source": [
    "# Build computational graph\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # input place holders\n",
    "    # input Shape: [# training examples, sequence length, # features]\n",
    "    x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "    print(\"input shape:\", x.shape)\n",
    "    \n",
    "    # label Shape: [# training examples, sequence length]\n",
    "    y = tf.placeholder(tf.int32,[None,seq_size])\n",
    "    print(\"ground truth shape:\", y.shape)\n",
    "    \n",
    "    num_examples = tf.shape(x)[0]\n",
    "    \n",
    "    # RNN Network\n",
    "    cell = rnn.BasicRNNCell(hidden_dim)\n",
    "    \n",
    "    # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "    print(\"output from tf,nn.dynamic_rnn shape\", outputs.shape)\n",
    "    \n",
    "    #outputs = tf.reshape(outputs, [num_examples*seq_size, hidden_dim])\n",
    "    print(\"reshaped output from RNN shape:\", outputs.shape)\n",
    "    \n",
    "    # weights for output dense layer (i.e., after RNN)\n",
    "    # W shape: [# hidden, 1]\n",
    "    W_out = tf.Variable(tf.random_normal([hidden_dim,size_vocab]),name=\"w_out\")\n",
    "    print(\"weights shape:\", W_out.shape)\n",
    "    # b shape: [1]\n",
    "    b_out = tf.Variable(tf.random_normal([size_vocab]),name=\"b_out\")\n",
    "    print(\"bias shape\",b_out.shape)\n",
    "    \n",
    "    # logit shape [# training examples, vocab_size] \n",
    "    outputs_reshaped = tf.reshape(outputs, [num_examples*seq_size, hidden_dim])\n",
    "    print(\"fully connected and reshaped outputs:\", outputs_reshaped) \n",
    "    \n",
    "    network_output = ((tf.matmul(outputs_reshaped,W_out) + b_out))\n",
    "    #print(network_output)\n",
    "          \n",
    "    y_pred = tf.reshape(\n",
    "        network_output,\n",
    "        [num_examples, seq_size, size_vocab])\n",
    "    print(\"final logits shape:\", y_pred.shape)\n",
    "    \n",
    "    # output dense layer:\n",
    "    # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "    # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "    #w_exp= tf.expand_dims(W_out,0)\n",
    "    # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "    #W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "    \n",
    "    # Dense Layer calculation: \n",
    "    # [# training examples, sequence length, # hidden] *\n",
    "    # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "    \n",
    "    # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "    # Remove last dimension using tf.squeeze\n",
    "    #y_pred = tf.add(tf.matmul(logit, W_repeated),b_out)\n",
    "    #y_pred = tf.squeeze(y_pred)    # [# training examples, sequence length]\n",
    "    #y_pred = tf.reshape(y_pred)\n",
    "    \n",
    "    #num_examples= tf.cast(num_examples, tf.float32)\n",
    "    \n",
    "    # Cost & Training Step\n",
    "    #cost = -tf.reduce_sum(y*tf.log(y_pred)) #cross_entropy\n",
    "    weights = tf.ones([num_examples,seq_size])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(y_pred,y,weights) \n",
    "    #cost = tf.reduce_sum(loss)/num_examples\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
=======
   "execution_count": 60,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
<<<<<<< HEAD
      "  step, train err=      0: 32.20824\n",
      "  step, train err=    100:  5.65707\n",
      "  step, train err=    200:  5.46456\n",
      "  step, train err=    300:  5.28226\n",
      "  step, train err=    400:  5.07700\n",
      "  step, train err=    500:  4.93164\n",
      "  step, train err=    600:  4.96913\n",
      "  step, train err=    700:  4.66821\n",
      "  step, train err=    800:  4.66383\n",
      "  step, train err=    900:  4.45866\n",
      "  step, train err=   1000:  4.38027\n"
=======
      "  step, train err=      0: 22.28258\n",
      "  step, train err=    100:  4.23270\n",
      "  step, train err=    200:  3.87242\n",
      "  step, train err=    300:  3.54585\n",
      "  step, train err=    400:  3.30021\n",
      "  step, train err=    500:  3.09255\n",
      "  step, train err=    600:  2.96147\n",
      "  step, train err=    700:  2.79036\n",
      "  step, train err=    800:  2.66798\n",
      "  step, train err=    900:  2.56860\n",
      "  step, train err=   1000:  2.46376\n"
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     ]
    }
   ],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(1000): # If we train more, would we overfit? Try 10000\n",
    "        _, train_err = sess.run([train_op,loss],feed_dict={x:trainX,y:trainY})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "\n",
    "    # Test trained model on training data\n",
    "    predicted_vals_all= sess.run(y_pred,feed_dict={x:trainX}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm=predicted_vals*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual=dataset[seq_size:][:,0]\n",
    "    # step 3: compute MSE\n",
    "    mse = ((predicted_vals_dnorm - actual) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals))), predicted_vals, color='r', label='predicted')\n",
    "    plt.plot(list(range(len(small_train))), small_train, color='g', label='actual')\n",
    "    plt.legend()\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": 61,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[[-11.572848  ,  -6.328885  ,  14.034     , ..., -16.505495  ,\n",
       "          -8.811107  , -10.090842  ],\n",
       "        [  1.2975172 , -12.755613  ,  12.964336  , ..., -14.244356  ,\n",
       "         -10.741328  ,  -2.6123343 ]],\n",
       "\n",
       "       [[-14.919275  ,  -3.9591277 ,  11.536594  , ..., -14.802935  ,\n",
       "          -8.9666195 ,  -9.356762  ],\n",
       "        [ -5.3952155 , -13.335586  ,  14.932098  , ..., -11.7702    ,\n",
       "          -6.7592    ,   0.38358665]],\n",
       "\n",
       "       [[-17.825146  ,  -2.4386344 ,   8.838403  , ..., -12.729684  ,\n",
       "          -9.1287155 ,  -9.708984  ],\n",
       "        [-14.334012  , -14.454574  ,  10.611996  , ...,  -6.6717777 ,\n",
       "          -4.6813755 ,  -2.528275  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-15.969505  ,   5.8199854 ,  -5.967566  , ...,  -2.430454  ,\n",
       "          -6.267274  ,  -7.075877  ],\n",
       "        [ -6.5880146 ,  16.08243   ,  -3.577097  , ...,  12.210869  ,\n",
       "          13.785131  ,  19.264702  ]],\n",
       "\n",
       "       [[ -7.572203  ,  16.54691   ,  -5.956917  , ...,  10.842388  ,\n",
       "          11.346909  ,  14.358203  ],\n",
       "        [ -6.1759315 ,  18.883236  ,  -5.642363  , ...,  17.940968  ,\n",
       "          17.931305  ,  15.971107  ]],\n",
       "\n",
       "       [[ -6.1737247 ,  18.886131  ,  -5.641292  , ...,  17.950472  ,\n",
       "          17.939898  ,  15.973811  ],\n",
       "        [-11.419944  ,  17.785131  ,  -8.619532  , ...,   6.736989  ,\n",
       "           4.1986637 ,   9.298856  ]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
=======
       "array([[  4.6802745,  -1.7353716,  17.095583 , ...,  -4.7615824,\n",
       "        -18.989502 , -19.83434  ],\n",
       "       [ -2.344129 ,   2.4373894,  10.094554 , ...,  -1.988703 ,\n",
       "        -21.116772 , -15.404271 ],\n",
       "       [ -4.502573 ,   4.04222  ,   5.1946573, ...,   4.851173 ,\n",
       "        -22.491318 , -13.500507 ],\n",
       "       ...,\n",
       "       [-20.098831 ,  11.615076 , -61.338345 , ...,  19.855085 ,\n",
       "         -6.8623915,  14.728275 ],\n",
       "       [-16.976099 ,   9.182478 , -56.451054 , ...,  -5.617437 ,\n",
       "          6.880569 ,  21.559963 ],\n",
       "       [-20.098831 ,  11.615076 , -61.338345 , ...,  19.855087 ,\n",
       "         -6.8623934,  14.728276 ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "predicted_vals_all"
=======
    "predicted_vals"
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.2975172 , -12.755613  ,  12.964336  , ..., -14.244356  ,\n",
       "        -10.741328  ,  -2.6123343 ],\n",
       "       [ -5.3952155 , -13.335586  ,  14.932098  , ..., -11.7702    ,\n",
       "         -6.7592    ,   0.38358665],\n",
       "       [-14.334012  , -14.454574  ,  10.611996  , ...,  -6.6717777 ,\n",
       "         -4.6813755 ,  -2.528275  ],\n",
       "       ...,\n",
       "       [ -6.5880146 ,  16.08243   ,  -3.577097  , ...,  12.210869  ,\n",
       "         13.785131  ,  19.264702  ],\n",
       "       [ -6.1759315 ,  18.883236  ,  -5.642363  , ...,  17.940968  ,\n",
       "         17.931305  ,  15.971107  ],\n",
       "       [-11.419944  ,  17.785131  ,  -8.619532  , ...,   6.736989  ,\n",
       "          4.1986637 ,   9.298856  ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_vals"
   ]
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> e9b1c92d829c6ef45bd20c4fbe1adfeb86928249
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
