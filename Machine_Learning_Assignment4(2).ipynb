{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read data & basic text prerpocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import shutil as sh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data/\"\n",
    "#path2 = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data_clean/\"\n",
    "\n",
    "path = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data/\"\n",
    "path2 = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocess(path,thefile):\n",
    "    # Replace all numbers with \"NUM\" special character\n",
    "    num_dec = re.compile(\"[\\d]+\\.[\\d]+\")\n",
    "    num_comma = re.compile(\"[\\d]+,[\\d]+\")\n",
    "    num_reg = re.compile(\"[\\d]+\")\n",
    "\n",
    "    # Detect non-English characters\n",
    "    non_eng = re.compile(\"[^a-zA-Z\\n ]\")\n",
    "    \n",
    "    #open file\n",
    "    file=open(os.path.join(path,thefile), \"r\", encoding=\"iso-8859-15\")\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    \n",
    "    # Replace various forms of numbers with NUM character\n",
    "    text = num_dec.sub(\"NUM\", text)\n",
    "    text = num_comma.sub(\"NUM\", text)\n",
    "    text = num_reg.sub(\"NUM\", text)\n",
    "\n",
    "    # Replace all non-English characters with a space\n",
    "    text = non_eng.sub(\" \", text)\n",
    "\n",
    "    # Replace double spaces with a single space\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    \n",
    "    # Split the file into reviews based on \\n\n",
    "    text = text.split(\"\\n\")\n",
    "        \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train=data_preprocess(path,\"down_sampled_reviews/train_tiny.txt\")\n",
    "valid=data_preprocess(path,\"down_sampled_reviews/valid_tiny.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create vocabulary\n",
    "\n",
    "reference source:https://stackoverflow.com/questions/40661684/tensorflow-vocabularyprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear netgear switch is over NUM and this one is under NUM both of them are all gigabit ports linksys has NUM gigabit ports and netgear has NUM gigabit ports however this srwNUM has more functions and features than the netgear NUM switches for example this switch supports lags for gang ports together in different lacp groups netgear switch does not this switch allowing setup stp on top the lags netgear does not this switch loaded with current firmware netgear switch comes with a two years old firmware this switch is really good bargain for its functions features and speed it is a shining star comparing to all those overpriced competitors the only problem we have is that this switch generates some kind of apple talk probing when we have switch setup to do stp on top of lags that probing caused problem to our routers and vpn servers so we had to turn the stp off  END what needs to be said really that hasn t already been said a million times this is one of the greatest albums of all time  and arguably dylan s best i say it is but everyone has their favorites and with an artist like dylan it s hard to pin down one album as his best either way  if you dont own this you should be ashamed take a hint from jack black in high fidelity  if you dont own it dont let anyone know as for the audiophile discussion that s been popping up  this sacd hybrid sounds spectacular and if you re one of those people that really like good sound it s worth a repurchase even if it is a bit pricey however if you truly want the best sound for this album  go with the NUM gram mono mix reissue from sundazed vinyl on a good turntable nothing digital can hold a candle to it but whatever your poison this is a timeless classic that only gets better every time you hear it six words of advice  buy it buy it buy it  END ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_reviews=2\n",
    "small_train=''\n",
    "for i in range(num_of_reviews):\n",
    "    small_train+=train[i]+\" END \"\n",
    "small_train=[small_train]\n",
    "small_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_length=len(small_train[0].split(\" \"))\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  93,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  93,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform the documents using the vocabulary.\n",
    "train_transformed = np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  93,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  93])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed=[np.trim_zeros(train_transformed[0], 'b')]\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<UNK>', 0), ('we', 1), ('brought', 2), ('two', 3), ('different', 4), ('brand', 5), ('switches', 6), ('to', 7), ('test', 8), ('this', 9), ('srwNUM', 10), ('from', 11), ('linksys', 12), ('and', 13), ('wsrNUM', 14), ('netgear', 15), ('switch', 16), ('is', 17), ('over', 18), ('NUM', 19), ('one', 20), ('under', 21), ('both', 22), ('of', 23), ('them', 24), ('are', 25), ('all', 26), ('gigabit', 27), ('ports', 28), ('has', 29), ('however', 30), ('more', 31), ('functions', 32), ('features', 33), ('than', 34), ('the', 35), ('for', 36), ('example', 37), ('supports', 38), ('lags', 39), ('gang', 40), ('together', 41), ('in', 42), ('lacp', 43), ('groups', 44), ('does', 45), ('not', 46), ('allowing', 47), ('setup', 48), ('stp', 49), ('on', 50), ('top', 51), ('loaded', 52), ('with', 53), ('current', 54), ('firmware', 55), ('comes', 56), ('a', 57), ('years', 58), ('old', 59), ('really', 60), ('good', 61), ('bargain', 62), ('its', 63), ('speed', 64), ('it', 65), ('shining', 66), ('star', 67), ('comparing', 68), ('those', 69), ('overpriced', 70), ('competitors', 71), ('only', 72), ('problem', 73), ('have', 74), ('that', 75), ('generates', 76), ('some', 77), ('kind', 78), ('apple', 79), ('talk', 80), ('probing', 81), ('when', 82), ('do', 83), ('caused', 84), ('our', 85), ('routers', 86), ('vpn', 87), ('servers', 88), ('so', 89), ('had', 90), ('turn', 91), ('off', 92), ('END', -1), ('what', 94), ('needs', 95), ('be', 96), ('said', 97), ('hasn', 98), ('t', 99), ('already', 100), ('been', 101), ('million', 102), ('times', 103), ('greatest', 104), ('albums', 105), ('time', 106), ('arguably', 107), ('dylan', 108), ('s', 109), ('best', 110), ('i', 111), ('say', 112), ('but', 113), ('everyone', 114), ('their', 115), ('favorites', 116), ('an', 117), ('artist', 118), ('like', 119), ('hard', 120), ('pin', 121), ('down', 122), ('album', 123), ('as', 124), ('his', 125), ('either', 126), ('way', 127), ('if', 128), ('you', 129), ('dont', 130), ('own', 131), ('should', 132), ('ashamed', 133), ('take', 134), ('hint', 135), ('jack', 136), ('black', 137), ('high', 138), ('fidelity', 139), ('let', 140), ('anyone', 141), ('know', 142), ('audiophile', 143), ('discussion', 144), ('popping', 145), ('up', 146), ('sacd', 147), ('hybrid', 148), ('sounds', 149), ('spectacular', 150), ('re', 151), ('people', 152), ('sound', 153), ('worth', 154), ('repurchase', 155), ('even', 156), ('bit', 157), ('pricey', 158), ('truly', 159), ('want', 160), ('go', 161), ('gram', 162), ('mono', 163), ('mix', 164), ('reissue', 165), ('sundazed', 166), ('vinyl', 167), ('turntable', 168), ('nothing', 169), ('digital', 170), ('can', 171), ('hold', 172), ('candle', 173), ('whatever', 174), ('your', 175), ('poison', 176), ('timeless', 177), ('classic', 178), ('gets', 179), ('better', 180), ('every', 181), ('hear', 182), ('six', 183), ('words', 184), ('advice', 185), ('buy', 186)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict['END'] = -1\n",
    "vocab_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3,  58,  59,  55,   9,\n",
       "         16,  17,  60,  61,  62,  36,  63,  32,  33,  13,  64,  65,  17,\n",
       "         57,  66,  67,  68,   7,  26,  69,  70,  71,  35,  72,  73,   1,\n",
       "         74,  17,  75,   9,  16,  76,  77,  78,  23,  79,  80,  81,  82,\n",
       "          1,  74,  16,  48,   7,  83,  49,  50,  51,  23,  39,  75,  81,\n",
       "         84,  73,   7,  85,  86,  13,  87,  88,  89,   1,  90,   7,  91,\n",
       "         35,  49,  92,  -1,  94,  95,   7,  96,  97,  60,  75,  98,  99,\n",
       "        100, 101,  97,  57, 102, 103,   9,  17,  20,  23,  35, 104, 105,\n",
       "         23,  26, 106,  13, 107, 108, 109, 110, 111, 112,  65,  17, 113,\n",
       "        114,  29, 115, 116,  13,  53, 117, 118, 119, 108,  65, 109, 120,\n",
       "          7, 121, 122,  20, 123, 124, 125, 110, 126, 127, 128, 129, 130,\n",
       "        131,   9, 129, 132,  96, 133, 134,  57, 135,  11, 136, 137,  42,\n",
       "        138, 139, 128, 129, 130, 131,  65, 130, 140, 141, 142, 124,  36,\n",
       "         35, 143, 144,  75, 109, 101, 145, 146,   9, 147, 148, 149, 150,\n",
       "         13, 128, 129, 151,  20,  23,  69, 152,  75,  60, 119,  61, 153,\n",
       "         65, 109, 154,  57, 155, 156, 128,  65,  17,  57, 157, 158,  30,\n",
       "        128, 129, 159, 160,  35, 110, 153,  36,   9, 123, 161,  53,  35,\n",
       "         19, 162, 163, 164, 165,  11, 166, 167,  50,  57,  61, 168, 169,\n",
       "        170, 171, 172,  57, 173,   7,  65, 113, 174, 175, 176,   9,  17,\n",
       "         57, 177, 178,  75,  72, 179, 180, 181, 106, 129, 182,  65, 183,\n",
       "        184,  23, 185, 186,  65, 186,  65, 186,  65,  -1])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 is in \n",
    "train_transformed2=np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "train_transformed2=[np.trim_zeros(train_transformed2[0], 'b')]\n",
    "train_transformed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n"
     ]
    }
   ],
   "source": [
    "## Sort the vocabulary dictionary on the basis of values(id).\n",
    "## Both statements perform same task.\n",
    "#sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "## word with id i goes at index i of the list.\n",
    "vocabulary = list(list(zip(*sorted_vocab))[1])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1], [2]]\n",
      "[[2], [3]]\n",
      "[[3], [4]]\n",
      "[[4], [5]]\n",
      "[[5], [6]]\n",
      "Y:\n",
      " [2 3]\n",
      "[3 4]\n",
      "[4 5]\n",
      "[5 6]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "seq_size = 2\n",
    "trainX, trainY = [], []\n",
    "max_document_length=len(train_transformed2[0])\n",
    "for i in range(max_document_length): \n",
    "    #[0,1,2,3,4,5,-1,1,2,3,4,5]\n",
    "    if -1 not in train_transformed2[0][i:i+seq_size+1]:\n",
    "        trainX.append(np.expand_dims(train_transformed2[0][i:i+seq_size], axis=1).tolist())\n",
    "        trainY.append(train_transformed2[0][i+1:i+seq_size+1])\n",
    "\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:5]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_vocab = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b Create vocabulary through a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textlst_2_vocab(textlst,num_of_reviews=None):\n",
    "    if num_of_reviews is None:\n",
    "        num_of_reviews=len(textlst)\n",
    "        \n",
    "    small_train=''\n",
    "    for i in range(num_of_reviews):\n",
    "        small_train+=textlst[i]+\" END \"\n",
    "    small_train=[small_train]\n",
    "    \n",
    "    max_document_length=len(small_train[0].split(\" \"))\n",
    "    \n",
    "    ## Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    \n",
    "    ## Transform the documents using the vocabulary.\n",
    "    train_transformed = np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "    \n",
    "    ## Extract word:id mapping from the object.\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    vocab_dict['END'] = -1\n",
    "    \n",
    "    train_transformed2=np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "    train_transformed2=[np.trim_zeros(train_transformed2[0], 'b')]\n",
    "    \n",
    "    #sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "    sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "    ## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "    ## word with id i goes at index i of the list.\n",
    "    vocabulary = list(list(zip(*sorted_vocab))[1])\n",
    "    \n",
    "    seq_size = 2\n",
    "    trainX, trainY = [], []\n",
    "    max_document_length=len(train_transformed2[0])\n",
    "    for i in range(max_document_length): \n",
    "        #[0,1,2,3,4,5,-1,1,2,3,4,5]\n",
    "        if -1 not in train_transformed2[0][i:i+seq_size+1]:\n",
    "            trainX.append(np.expand_dims(train_transformed2[0][i:i+seq_size], axis=1).tolist())\n",
    "            trainY.append(train_transformed2[0][i+1:i+seq_size+1])\n",
    "    \n",
    "    size_vocab = len(vocabulary)\n",
    "    \n",
    "    return trainX,trainY,size_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32099\n",
      "X:\n",
      " [[1], [2]]\n",
      "[[2], [3]]\n",
      "[[3], [4]]\n",
      "[[4], [5]]\n",
      "[[5], [6]]\n",
      "Y:\n",
      " [2 3]\n",
      "[3 4]\n",
      "[4 5]\n",
      "[5 6]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,size_vocab=textlst_2_vocab(train)\n",
    "print(size_vocab)\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:5]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (?, 2, 1)\n",
      "ground truth shape: (?, 2)\n",
      "output from tf,nn.dynamic_rnn shape (?, 2, 100)\n",
      "reshaped output from RNN shape: (?, 2, 100)\n",
      "weights shape: (100, 187)\n",
      "bias shape (187,)\n",
      "fully connected and reshaped outputs: Tensor(\"Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "final logits shape: (?, 2, 187)\n"
     ]
    }
   ],
   "source": [
    "# Build computational graph\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # input place holders\n",
    "    # input Shape: [# training examples, sequence length, # features]\n",
    "    x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "    print(\"input shape:\", x.shape)\n",
    "    \n",
    "    # label Shape: [# training examples, sequence length]\n",
    "    y = tf.placeholder(tf.int32,[None,seq_size])\n",
    "    print(\"ground truth shape:\", y.shape)\n",
    "    \n",
    "    num_examples = tf.shape(x)[0]\n",
    "    \n",
    "    # RNN Network\n",
    "    cell = rnn.BasicRNNCell(hidden_dim)\n",
    "    \n",
    "    # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "    print(\"output from tf,nn.dynamic_rnn shape\", outputs.shape)\n",
    "    \n",
    "    #outputs = tf.reshape(outputs, [num_examples*seq_size, hidden_dim])\n",
    "    print(\"reshaped output from RNN shape:\", outputs.shape)\n",
    "    \n",
    "    # weights for output dense layer (i.e., after RNN)\n",
    "    # W shape: [# hidden, 1]\n",
    "    W_out = tf.Variable(tf.random_normal([hidden_dim,size_vocab]),name=\"w_out\")\n",
    "    print(\"weights shape:\", W_out.shape)\n",
    "    # b shape: [1]\n",
    "    b_out = tf.Variable(tf.random_normal([size_vocab]),name=\"b_out\")\n",
    "    print(\"bias shape\",b_out.shape)\n",
    "    \n",
    "    # logit shape [# training examples, vocab_size] \n",
    "    outputs_reshaped = tf.reshape(outputs, [num_examples*seq_size, hidden_dim])\n",
    "    print(\"fully connected and reshaped outputs:\", outputs_reshaped) \n",
    "    \n",
    "    network_output = ((tf.matmul(outputs_reshaped,W_out) + b_out))\n",
    "    #print(network_output)\n",
    "          \n",
    "    y_pred = tf.reshape(\n",
    "        network_output,\n",
    "        [num_examples, seq_size, size_vocab])\n",
    "    print(\"final logits shape:\", y_pred.shape)\n",
    "    \n",
    "    # output dense layer:\n",
    "    # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "    # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "    #w_exp= tf.expand_dims(W_out,0)\n",
    "    # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "    #W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "    \n",
    "    # Dense Layer calculation: \n",
    "    # [# training examples, sequence length, # hidden] *\n",
    "    # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "    \n",
    "    # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "    # Remove last dimension using tf.squeeze\n",
    "    #y_pred = tf.add(tf.matmul(logit, W_repeated),b_out)\n",
    "    #y_pred = tf.squeeze(y_pred)    # [# training examples, sequence length]\n",
    "    #y_pred = tf.reshape(y_pred)\n",
    "    \n",
    "    #num_examples= tf.cast(num_examples, tf.float32)\n",
    "    \n",
    "    # Cost & Training Step\n",
    "    #cost = -tf.reduce_sum(y*tf.log(y_pred)) #cross_entropy\n",
    "    weights = tf.ones([num_examples,seq_size])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(y_pred,y,weights) \n",
    "    #cost = tf.reduce_sum(loss)/num_examples\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  step, train err=      0: 22.28258\n",
      "  step, train err=    100:  4.23270\n",
      "  step, train err=    200:  3.87242\n",
      "  step, train err=    300:  3.54585\n",
      "  step, train err=    400:  3.30021\n",
      "  step, train err=    500:  3.09255\n",
      "  step, train err=    600:  2.96147\n",
      "  step, train err=    700:  2.79036\n",
      "  step, train err=    800:  2.66798\n",
      "  step, train err=    900:  2.56860\n",
      "  step, train err=   1000:  2.46376\n"
     ]
    }
   ],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(1000): # If we train more, would we overfit? Try 10000\n",
    "        _, train_err = sess.run([train_op,loss],feed_dict={x:trainX,y:trainY})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "\n",
    "    # Test trained model on training data\n",
    "    predicted_vals_all= sess.run(y_pred,feed_dict={x:trainX}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm=predicted_vals*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual=dataset[seq_size:][:,0]\n",
    "    # step 3: compute MSE\n",
    "    mse = ((predicted_vals_dnorm - actual) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals))), predicted_vals, color='r', label='predicted')\n",
    "    plt.plot(list(range(len(small_train))), small_train, color='g', label='actual')\n",
    "    plt.legend()\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.6802745,  -1.7353716,  17.095583 , ...,  -4.7615824,\n",
       "        -18.989502 , -19.83434  ],\n",
       "       [ -2.344129 ,   2.4373894,  10.094554 , ...,  -1.988703 ,\n",
       "        -21.116772 , -15.404271 ],\n",
       "       [ -4.502573 ,   4.04222  ,   5.1946573, ...,   4.851173 ,\n",
       "        -22.491318 , -13.500507 ],\n",
       "       ...,\n",
       "       [-20.098831 ,  11.615076 , -61.338345 , ...,  19.855085 ,\n",
       "         -6.8623915,  14.728275 ],\n",
       "       [-16.976099 ,   9.182478 , -56.451054 , ...,  -5.617437 ,\n",
       "          6.880569 ,  21.559963 ],\n",
       "       [-20.098831 ,  11.615076 , -61.338345 , ...,  19.855087 ,\n",
       "         -6.8623934,  14.728276 ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
