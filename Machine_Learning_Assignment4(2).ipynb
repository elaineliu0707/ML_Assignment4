{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Jo's trial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read data & basic text prerpocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import shutil as sh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data/\"\n",
    "path2 = \"/home/yingjie/Desktop/Machine_Learning/Assignment4/data_clean/\"\n",
    "\n",
    "#path = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data/\"\n",
    "#path2 = \"/Users/Pan/Google Drive/Data Science/Machine Learning/ML_Assignment4/data_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocess(path,thefile):\n",
    "    # Replace all numbers with \"NUM\" special character\n",
    "    num_dec = re.compile(\"[\\d]+\\.[\\d]+\")\n",
    "    num_comma = re.compile(\"[\\d]+,[\\d]+\")\n",
    "    num_reg = re.compile(\"[\\d]+\")\n",
    "\n",
    "    # Detect non-English characters\n",
    "    non_eng = re.compile(\"[^a-zA-Z\\n ]\")\n",
    "    \n",
    "    #open file\n",
    "    file=open(os.path.join(path,thefile), \"r\", encoding=\"iso-8859-15\")\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    \n",
    "    # Replace various forms of numbers with NUM character\n",
    "    text = num_dec.sub(\"NUM\", text)\n",
    "    text = num_comma.sub(\"NUM\", text)\n",
    "    text = num_reg.sub(\"NUM\", text)\n",
    "\n",
    "    # Replace all non-English characters with a space\n",
    "    text = non_eng.sub(\" \", text)\n",
    "\n",
    "    # Replace double spaces with a single space\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    \n",
    "    # Split the file into reviews based on \\n\n",
    "    text = text.split(\"\\n\")\n",
    "        \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train=data_preprocess(path,\"train.txt\")\n",
    "valid=data_preprocess(path,\"valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create vocabulary\n",
    "\n",
    "reference source:https://stackoverflow.com/questions/40661684/tensorflow-vocabularyprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingjie/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2 = [x + \" END\" for x in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we brought two different brand switches to test this srwNUM from linksys and wsrNUM from netgear netgear switch is over NUM and this one is under NUM both of them are all gigabit ports linksys has NUM gigabit ports and netgear has NUM gigabit ports however this srwNUM has more functions and features than the netgear NUM switches for example this switch supports lags for gang ports together in different lacp groups netgear switch does not this switch allowing setup stp on top the lags netgear does not this switch loaded with current firmware netgear switch comes with a two years old firmware this switch is really good bargain for its functions features and speed it is a shining star comparing to all those overpriced competitors the only problem we have is that this switch generates some kind of apple talk probing when we have switch setup to do stp on top of lags that probing caused problem to our routers and vpn servers so we had to turn the stp off  END',\n",
       " 'what needs to be said really that hasn t already been said a million times this is one of the greatest albums of all time  and arguably dylan s best i say it is but everyone has their favorites and with an artist like dylan it s hard to pin down one album as his best either way  if you dont own this you should be ashamed take a hint from jack black in high fidelity  if you dont own it dont let anyone know as for the audiophile discussion that s been popping up  this sacd hybrid sounds spectacular and if you re one of those people that really like good sound it s worth a repurchase even if it is a bit pricey however if you truly want the best sound for this album  go with the NUM gram mono mix reissue from sundazed vinyl on a good turntable nothing digital can hold a candle to it but whatever your poison this is a timeless classic that only gets better every time you hear it six words of advice  buy it buy it buy it  END']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train=train2[0:2]\n",
    "small_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#max_document_length = max([len(small_train.split(\" \"))])\n",
    "max_document_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  11,  15,  15,  16,  17,  18,  19,  13,   9,  20,  17,  21,\n",
       "         19,  22,  23,  24,  25,  26,  27,  28,  12,  29,  19,  27,  28,\n",
       "         13,  15,  29,  19,  27,  28,  30,   9,  10,  29,  31,  32,  13,\n",
       "         33,  34,  35,  15,  19,   6,  36,  37,   9,  16,  38,  39,  36,\n",
       "         40,  28,  41,  42,   4,  43,  44,  15,  16,  45,  46,   9,  16,\n",
       "         47,  48,  49,  50,  51,  35,  39,  15,  45,  46,   9,  16,  52,\n",
       "         53,  54,  55,  15,  16,  56,  53,  57,   3],\n",
       "       [ 94,  95,   7,  96,  97,  60,  75,  98,  99, 100, 101,  97,  57,\n",
       "        102, 103,   9,  17,  20,  23,  35, 104, 105,  23,  26, 106,  13,\n",
       "        107, 108, 109, 110, 111, 112,  65,  17, 113, 114,  29, 115, 116,\n",
       "         13,  53, 117, 118, 119, 108,  65, 109, 120,   7, 121, 122,  20,\n",
       "        123, 124, 125, 110, 126, 127, 128, 129, 130, 131,   9, 129, 132,\n",
       "         96, 133, 134,  57, 135,  11, 136, 137,  42, 138, 139, 128, 129,\n",
       "        130, 131,  65, 130, 140, 141, 142, 124,  36,  35, 143, 144,  75,\n",
       "        109, 101, 145, 146,   9, 147, 148, 149, 150]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform the documents using the vocabulary.\n",
    "train_transformed = np.array(list(vocab_processor.fit_transform(small_train)))\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=vocab_processor.fit_transform(small_train)\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "#np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('<UNK>', 0), ('we', 1), ('brought', 2), ('two', 3), ('different', 4), ('brand', 5), ('switches', 6), ('to', 7), ('test', 8), ('this', 9), ('srwNUM', 10), ('from', 11), ('linksys', 12), ('and', 13), ('wsrNUM', 14), ('netgear', 15), ('switch', 16), ('is', 17), ('over', 18), ('NUM', 19), ('one', 20), ('under', 21), ('both', 22), ('of', 23), ('them', 24), ('are', 25), ('all', 26), ('gigabit', 27), ('ports', 28), ('has', 29), ('however', 30), ('more', 31), ('functions', 32), ('features', 33), ('than', 34), ('the', 35), ('for', 36), ('example', 37), ('supports', 38), ('lags', 39), ('gang', 40), ('together', 41), ('in', 42), ('lacp', 43), ('groups', 44), ('does', 45), ('not', 46), ('allowing', 47), ('setup', 48), ('stp', 49), ('on', 50), ('top', 51), ('loaded', 52), ('with', 53), ('current', 54), ('firmware', 55), ('comes', 56), ('a', 57), ('years', 58), ('old', 59), ('really', 60), ('good', 61), ('bargain', 62), ('its', 63), ('speed', 64), ('it', 65), ('shining', 66), ('star', 67), ('comparing', 68), ('those', 69), ('overpriced', 70), ('competitors', 71), ('only', 72), ('problem', 73), ('have', 74), ('that', 75), ('generates', 76), ('some', 77), ('kind', 78), ('apple', 79), ('talk', 80), ('probing', 81), ('when', 82), ('do', 83), ('caused', 84), ('our', 85), ('routers', 86), ('vpn', 87), ('servers', 88), ('so', 89), ('had', 90), ('turn', 91), ('off', 92), ('END', -1), ('what', 94), ('needs', 95), ('be', 96), ('said', 97), ('hasn', 98), ('t', 99), ('already', 100), ('been', 101), ('million', 102), ('times', 103), ('greatest', 104), ('albums', 105), ('time', 106), ('arguably', 107), ('dylan', 108), ('s', 109), ('best', 110), ('i', 111), ('say', 112), ('but', 113), ('everyone', 114), ('their', 115), ('favorites', 116), ('an', 117), ('artist', 118), ('like', 119), ('hard', 120), ('pin', 121), ('down', 122), ('album', 123), ('as', 124), ('his', 125), ('either', 126), ('way', 127), ('if', 128), ('you', 129), ('dont', 130), ('own', 131), ('should', 132), ('ashamed', 133), ('take', 134), ('hint', 135), ('jack', 136), ('black', 137), ('high', 138), ('fidelity', 139), ('let', 140), ('anyone', 141), ('know', 142), ('audiophile', 143), ('discussion', 144), ('popping', 145), ('up', 146), ('sacd', 147), ('hybrid', 148), ('sounds', 149), ('spectacular', 150), ('re', 151), ('people', 152), ('sound', 153), ('worth', 154), ('repurchase', 155), ('even', 156), ('bit', 157), ('pricey', 158), ('truly', 159), ('want', 160), ('go', 161), ('gram', 162), ('mono', 163), ('mix', 164), ('reissue', 165), ('sundazed', 166), ('vinyl', 167), ('turntable', 168), ('nothing', 169), ('digital', 170), ('can', 171), ('hold', 172), ('candle', 173), ('whatever', 174), ('your', 175), ('poison', 176), ('timeless', 177), ('classic', 178), ('gets', 179), ('better', 180), ('every', 181), ('hear', 182), ('six', 183), ('words', 184), ('advice', 185), ('buy', 186)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['END'] = -1\n",
    "vocab_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort the vocabulary dictionary on the basis of values(id).\n",
    "## Both statements perform same task.\n",
    "#sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n"
     ]
    }
   ],
   "source": [
    "## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "## word with id i goes at index i of the list.\n",
    "vocabulary = list(list(zip(*sorted_vocab))[1])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1], [2]]\n",
      "[[2], [3]]\n",
      "[[3], [4]]\n",
      "[[4], [5]]\n",
      "[[5], [6]]\n",
      "Y:\n",
      " [2 3]\n",
      "[3 4]\n",
      "[4 5]\n",
      "[5 6]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "seq_size = 2\n",
    "trainX, trainY = [], []\n",
    "\n",
    "for review in train_transformed:\n",
    "    #loop through every word in the review\n",
    "    for i in range(max_document_length):        \n",
    "        #if the next 10 consecutive words are 0, that means the review has ended.\n",
    "        # eg: [1 2 3 4 0 0 0 0]\n",
    "        # this will create x: [1 2] [2 3]. (no [3 4])\n",
    "        \n",
    "        if all(review[i+seq_size:i+seq_size+10] ==0):        \n",
    "            break\n",
    "        else:\n",
    "            trainX.append(np.expand_dims(review[i:i+seq_size], axis=1).tolist())\n",
    "            trainY.append(review[i+1:i+seq_size+1])\n",
    "\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:5]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_vocab = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n",
      "(100, 187)\n",
      "(187,)\n",
      "(?, 2, 187)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 187 and 100 for 'MatMul' (op: 'BatchMatMul') with input shapes: [?,2,187], [?,100,187].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 187 and 100 for 'MatMul' (op: 'BatchMatMul') with input shapes: [?,2,187], [?,100,187].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-b15d679a208c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Actually, y_pred: [# training examples, sequence length, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Remove last dimension using tf.squeeze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_repeated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m#y_pred = tf.squeeze(y_pred)    # [# training examples, sequence length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#y_pred = tf.reshape(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m       return gen_math_ops._batch_mat_mul(\n\u001b[0;32m-> 1985\u001b[0;31m           a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;31m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_batch_mat_mul\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 737\u001b[0;31m         \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n\u001b[0m\u001b[1;32m    738\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 187 and 100 for 'MatMul' (op: 'BatchMatMul') with input shapes: [?,2,187], [?,100,187]."
     ]
    }
   ],
   "source": [
    "# Build computational graph\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # input place holders\n",
    "    # input Shape: [# training examples, sequence length, # features]\n",
    "    x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "    # label Shape: [# training examples, sequence length]\n",
    "    y = tf.placeholder(tf.float32,[None,seq_size])\n",
    "    \n",
    "    num_examples = tf.shape(x)[0]\n",
    "    \n",
    "    # RNN Network\n",
    "    cell = rnn.BasicRNNCell(hidden_dim)\n",
    "    \n",
    "    # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "    outputs = tf.reshape(outputs, [num_examples*seq_size, hidden_dim])\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    # weights for output dense layer (i.e., after RNN)\n",
    "    # W shape: [# hidden, 1]\n",
    "    W_out = tf.Variable(tf.ones([hidden_dim,size_vocab]),name=\"w_out\")\n",
    "    print(W_out.shape)\n",
    "    # b shape: [1]\n",
    "    b_out = tf.Variable(tf.random_normal([size_vocab]),name=\"b_out\")\n",
    "    print(b_out.shape)\n",
    "    \n",
    "    # logit shape [# training examples, vocab_size] \n",
    "    logit = tf.contrib.layers.fully_connected(outputs, size_vocab, activation_fn=None)\n",
    "    logit = tf.reshape(logit, [num_examples,seq_size, size_vocab])\n",
    "    print(logit.shape) #correct - (100, 2, 187)\n",
    "    \n",
    "    # output dense layer:\n",
    "    # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "    # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "    w_exp= tf.expand_dims(W_out,0)\n",
    "    # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "    W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "    \n",
    "    # Dense Layer calculation: \n",
    "    # [# training examples, sequence length, # hidden] *\n",
    "    # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "    \n",
    "    # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "    # Remove last dimension using tf.squeeze\n",
    "    y_pred = tf.add(tf.matmul(logit, W_repeated),b_out)\n",
    "    #y_pred = tf.squeeze(y_pred)    # [# training examples, sequence length]\n",
    "    #y_pred = tf.reshape(y_pred)\n",
    "    \n",
    "    \n",
    "    # Cost & Training Step\n",
    "    #cost = -tf.reduce_sum(y*tf.log(y_pred)) #cross_entropy\n",
    "    weights = tf.ones([num_examples*size_vocab])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logit,y,weights)\n",
    "    cost = tf.reduce_summ(loss)/num_examples\n",
    "    #train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(1000): # If we train more, would we overfit? Try 10000\n",
    "        _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "\n",
    "    # Test trained model on training data\n",
    "    predicted_vals_all= sess.run(y_pred,feed_dict={x:trainX}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm=predicted_vals*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual=dataset[seq_size:][:,0]\n",
    "    # step 3: compute MSE\n",
    "    mse = ((predicted_vals_dnorm - actual) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals_dnorm))), predicted_vals_dnorm, color='r', label='predicted')\n",
    "    plt.plot(list(range(len(dataset))), dataset, color='g', label='actual')\n",
    "    plt.legend()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_vals_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
